{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Train an All-Set-Transformer TNN\n",
    "\n",
    "In this notebook, we will create and train a two-step message passing network named AllSetTransformer (Chien et al., [2021](https://arxiv.org/abs/2106.13264)) in the hypergraph domain. We will use a benchmark dataset, shrec16, a collection of 3D meshes, to train the model to perform classification at the level of the hypergraph. \n",
    "\n",
    "Following the \"awesome-tnns\" [github repo.](https://github.com/awesome-tnns/awesome-tnns/blob/main/Hypergraphs.md)\n",
    "\n",
    "üüß $\\quad m_{\\rightarrow z}^{(\\rightarrow 1)} = AGG_{y \\in \\mathcal{B}(z)} (h_y^{t, (0)}, h_z^{t,(1)}) \\quad \\text{with attention}$ \n",
    "\n",
    "üü¶ $\\quad h_z^{t+1,(1)} = \\text{LN}(m_{\\rightarrow z}^{(\\rightarrow 1)} + \\text{MLP}(m_{\\rightarrow z}^{(\\rightarrow 1)} ))$ \n",
    "\n",
    "Edge to vertex: \n",
    "\n",
    "üüß $\\quad m_{\\rightarrow x}^{(\\rightarrow 0)} = AGG_{z \\in \\mathcal{C}(x)} (h_z^{t+1,(1)}, h_x^{t,(0)}) \\quad \\text{with attention}$ \n",
    "\n",
    "üü¶ $\\quad h_x^{t+1,(0)} = \\text{LN}(m_{\\rightarrow x}^{(\\rightarrow 0)} + \\text{MLP}(m_{\\rightarrow x}^{(\\rightarrow 0)} ))$\n",
    "\n",
    "\n",
    "\n",
    "### Additional theoretical clarifications\n",
    "\n",
    "Given a hypergraph $G=(\\mathcal{V}, \\mathcal{E})$, let $\\textbf{X} \\in \\mathbb{R}^{|\\mathcal{V}| \\times F}$ and $\\textbf{Z} \\in \\mathbb{R}^{|\\mathcal{E}| \\times F'}$ denote the hidden node and hyperedge representations, respectively. Additionally, define $V_{e, \\textbf{X}} = \\{\\textbf{X}_{u,:}: u \\in e\\}$ as the multiset of hidden node representations in the hyperedge $e$ and $E_{v, \\textbf{Z}} = \\{\\textbf{Z}_{e,:}: v \\in e\\}$ as the multiset of hidden representations of hyperedges containing $v$.\n",
    "\n",
    "\\\n",
    "In this setting, the two general update rules that AllSet's framework puts in place in each layer are:\n",
    "\n",
    "üî∑ $\\textbf{Z}_{e,:}^{(t+1)} = f_{\\mathcal{V} \\rightarrow \\mathcal{E}}(V_{e, \\textbf{X}^{(t)}}; \\textbf{Z}_{e,:}^{(t)})$\n",
    "\n",
    "üî∑ $\\textbf{X}_{v,:}^{(t+1)} = f_{\\mathcal{E} \\rightarrow \\mathcal{V}}(E_{v, \\textbf{Z}^{(t+1)}}; \\textbf{X}_{v,:}^{(t)})$\n",
    "\n",
    "in which $f_{\\mathcal{V} \\rightarrow \\mathcal{E}}$ and $f_{\\mathcal{E} \\rightarrow \\mathcal{V}}$ are two permutation invariant functions with respect to their first input. The matrices $\\textbf{Z}_{e,:}^{(0)}$ and $\\textbf{X}_{v,:}^{(0)}$ are initialized with the hyperedge and node features respectively, if available, otherwise they are set to be all-zero matrices.\n",
    "\n",
    "In the practical implementation of the model, $f_{\\mathcal{V} \\rightarrow \\mathcal{E}}$ and $f_{\\mathcal{E} \\rightarrow \\mathcal{V}}$ are parametrized and $learnt$ for each dataset and task, and the information of their second argument is not utilized. The option achieving the best results makes use of attention-based layers, giving rise to the so-called AllSetTransformer architecture.\n",
    "\n",
    "\\\n",
    "We now dive deep into the details of AllSetTransformer, describing how the update functions $f_{\\mathcal{V} \\rightarrow \\mathcal{E}}$ and $f_{\\mathcal{E} \\rightarrow \\mathcal{V}}$ are iteratively defined.\n",
    "Their input is a matrix $\\textbf{S} \\in \\mathbb{R}^{|S| \\times F}$ which corresponds the multiset of $F$-dimensional feature vectors:\n",
    "\n",
    "1Ô∏è‚É£ $\\textbf{K}^{(i)} = \\text{MLP}^{K, i}(\\textbf{S}), \\textbf{V}^{(i)} = \\text{MLP}^{V, i}(\\textbf{S})$, where $i \\in \\{1, ..., h\\},$\n",
    "\n",
    "2Ô∏è‚É£ $ \\textbf{O}^{(i)} = \\omega (\\theta^{(i)}(\\textbf{K}^{(i)})^{T}) \\textbf{V}^{(i)},$  \n",
    "\n",
    "3Ô∏è‚É£ $\\theta  \\overset{\\Delta}{=} \\mathbin\\Vert_{i=1}^{h} \\theta^{(i)}, $\n",
    "\n",
    "4Ô∏è‚É£ $ \\text{MH}_{h, \\omega}(\\theta, \\textbf{S}, \\textbf{S}) = \\mathbin\\Vert_{i=1}^{h} \\textbf{O}^{(i)}, $\n",
    "\n",
    "5Ô∏è‚É£ $ \\textbf{Y} = \\text{LN} (\\theta + \\text{MH}_{h, \\omega}(\\theta, \\textbf{S}, \\textbf{S})), $\n",
    "\n",
    "6Ô∏è‚É£ $f_{\\mathcal{V} \\rightarrow \\mathcal{E}}(\\textbf{S}) =  f_{\\mathcal{E} \\rightarrow \\mathcal{V}}(\\textbf{S}) = \\text{LN} (\\textbf{Y} + \\text{MLP}(\\textbf{Y}))$.\n",
    "\n",
    "\\\n",
    "\n",
    "The elements and operations used in these steps are defined as follows:\n",
    "\n",
    "üî∂ $\\text{LN}$ means layer normalization (Ba et al., [2016](https://arxiv.org/abs/1607.06450)),\n",
    "\n",
    "üî∂ $\\mathbin\\Vert$ represents concatenation,\n",
    "\n",
    "üî∂ $\\theta \\in \\mathbb{R}^{1 \\times hF_{h}}$ is a learnable weight,\n",
    "\n",
    "üî∂ $\\text{MH}_{h, \\omega}$ denotes a multihead attention mechanism with $h$ heads and activation function $\\omega$ (Vaswani et al., [2017](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)),\n",
    "\n",
    "üî∂ all $\\text{MLP}$ modules are multi-layer perceptrons that operate row-wise, so they are applied identically and independently to each multiset element of $\\textbf{S}$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import toponetx.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from topomodelx.nn.hypergraph.allset_transformer import AllSetTransformer\n",
    "from topomodelx.utils.sparse import from_sparse\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, shrec 16, a benchmark dataset for 3D mesh classification. We then lift each graph into our domain of choice, a hypergraph.\n",
    "\n",
    "We will also retrieve:\n",
    "- input signal on the edges for each of these hypergraphs, as that will be what we feed the model in input\n",
    "- the label associated to the hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:53.022151550Z",
     "start_time": "2023-06-01T16:14:52.949636599Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "import torch_geometric.datasets as geom_datasets\n",
    "cora = geom_datasets.Planetoid(root=\"/TopoModelX/data/cora\", name=\"Cora\")\n",
    "\n",
    "\n",
    "x_0s = cora.data.x\n",
    "edge_index = cora.data.edge_index\n",
    "\n",
    "y = cora.data.y\n",
    "\n",
    "# train_mask = cora.data.train_mask\n",
    "# val_mask = cora.data.val_mask\n",
    "# test_mask = cora.data.test_mask\n",
    "\n",
    "# make custom train test val split\n",
    "idxs = np.arange(len(y))\n",
    "train_idxs, test_idxs = train_test_split(idxs, test_size=0.2, random_state=42)\n",
    "train_idxs, val_idxs = train_test_split(train_idxs, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "train_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "train_mask[train_idxs] = True\n",
    "\n",
    "val_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "val_mask[val_idxs] = True\n",
    "\n",
    "test_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "test_mask[test_idxs] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperedges 5278\n",
      "False\n",
      "Number of isolated nodes 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "for edge in edge_index.numpy().T:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "\n",
    "print('Number of hyperedges', G.number_of_edges())\n",
    "# check if dirrected\n",
    "print(G.is_directed())\n",
    "\n",
    "#check if there is isolated nodes in the graph\n",
    "print('Number of isolated nodes', len(list(nx.isolates(G))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from toponetx.classes.simplicial_complex import SimplicialComplex\n",
    "# a = SimplicialComplex([[0,1,2,3], [3,4,5,6]])\n",
    "# a.to_hypergraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from toponetx.datasets.graph import coauthorship, karate_club\n",
    "# coa = coauthorship()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures and lift into hypergraph domain. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on each simplicial complex. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$.\n",
    "\n",
    "Once we have recorded the incidence matrix (note that all incidence amtrices in the hypergraph domain must be unsigned), we lift each simplicial complex into a hypergraph. The pairwise edges will become pairwise hyperedges, and faces in the simplciial complex will become 3-wise hyperedges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:53.022151550Z",
     "start_time": "2023-06-01T16:14:52.949636599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperedges 2708\n",
      "Number of hyperedges without duplicated hyperedges 2590\n",
      "Hyperedge statistics:     min = 2     max = 169     mean = 4.991891891891892     median = 4.0     std = 5.322452976509452\n"
     ]
    }
   ],
   "source": [
    "hyperedges = []\n",
    "for node in G.nodes():\n",
    "    hyperedge = sorted(list(G.neighbors(node)) + [node])\n",
    "    hyperedges.append(tuple(hyperedge))\n",
    "    \n",
    "        \n",
    "print('Number of hyperedges', len(hyperedges))\n",
    "\n",
    "# Delete duplicates\n",
    "hyperedges = list(set(hyperedges))\n",
    "print('Number of hyperedges without duplicated hyperedges', len(hyperedges))\n",
    "\n",
    "print(f'Hyperedge statistics: \\\n",
    "    min = {min([len(he) for he in hyperedges])} \\\n",
    "    max = {max([len(he) for he in hyperedges])} \\\n",
    "    mean = {np.mean([len(he) for he in hyperedges])} \\\n",
    "    median = {np.median([len(he) for he in hyperedges])} \\\n",
    "    std = {np.std([len(he) for he in hyperedges])}')\n",
    "\n",
    "# Construct hypergraph\n",
    "from hypernetx import Hypergraph\n",
    "hyperedges = {f'e{idx}':list(he) for idx, he in enumerate(hyperedges)}\n",
    "incidence_1 = Hypergraph(hyperedges, static=True).incidence_matrix()\n",
    "incidence_1 = from_sparse(incidence_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = x_0s.shape[1]\n",
    "hid_dim = 128\n",
    "out_dim = len(torch.unique(y))\n",
    "heads = 8\n",
    "n_layers = 1\n",
    "mlp_num_layers = 2\n",
    "#Q_n = 1\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = AllSetTransformer(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hid_dim,\n",
    "    heads=heads,\n",
    "    out_channels=out_dim,\n",
    "    n_layers=n_layers,\n",
    "    mlp_num_layers=mlp_num_layers,\n",
    "    dropout=0.25,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, the loss, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# Categorial cross-entropy loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Accuracy\n",
    "acc_fn = lambda y, y_hat: (y == y_hat).float().mean()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:59.046068930Z",
     "start_time": "2023-06-01T16:14:59.037648626Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_size = 0.2\n",
    "# x_0_train, x_0_test = train_test_split(x_0s, test_size=test_size, shuffle=False)\n",
    "# incidence_1_train, incidence_1_test = train_test_split(\n",
    "#     incidence_1_list, test_size=test_size, shuffle=False\n",
    "# )\n",
    "# y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112577/424802673.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_0s = torch.tensor(x_0s)\n",
      "/tmp/ipykernel_112577/424802673.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y, dtype=torch.long).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 2.0500,         acc: 0.1253\n",
      "Test_loss: 2.0209,                    Val_acc: 0.1359\n",
      "Epoch: 2 loss: 2.1502,         acc: 0.2552\n",
      "Test_loss: 2.1461,                    Val_acc: 0.2373\n",
      "Epoch: 3 loss: 2.0653,         acc: 0.1686\n",
      "Test_loss: 2.0859,                    Val_acc: 0.1682\n",
      "Epoch: 4 loss: 1.8959,         acc: 0.2841\n",
      "Test_loss: 1.9096,                    Val_acc: 0.2995\n",
      "Epoch: 5 loss: 1.9125,         acc: 0.2737\n",
      "Test_loss: 1.9260,                    Val_acc: 0.2558\n",
      "Epoch: 6 loss: 1.8842,         acc: 0.2812\n",
      "Test_loss: 1.8673,                    Val_acc: 0.2995\n",
      "Epoch: 7 loss: 1.8734,         acc: 0.2852\n",
      "Test_loss: 1.8388,                    Val_acc: 0.3018\n",
      "Epoch: 8 loss: 1.8695,         acc: 0.2846\n",
      "Test_loss: 1.8469,                    Val_acc: 0.3065\n",
      "Epoch: 9 loss: 1.8626,         acc: 0.2639\n",
      "Test_loss: 1.8628,                    Val_acc: 0.2581\n",
      "Epoch: 10 loss: 1.8571,         acc: 0.2725\n",
      "Test_loss: 1.8502,                    Val_acc: 0.2811\n",
      "Epoch: 11 loss: 1.8486,         acc: 0.2846\n",
      "Test_loss: 1.8501,                    Val_acc: 0.2949\n",
      "Epoch: 12 loss: 1.8420,         acc: 0.2864\n",
      "Test_loss: 1.8554,                    Val_acc: 0.3018\n",
      "Epoch: 13 loss: 1.8385,         acc: 0.2870\n",
      "Test_loss: 1.8349,                    Val_acc: 0.3041\n",
      "Epoch: 14 loss: 1.8201,         acc: 0.2921\n",
      "Test_loss: 1.8244,                    Val_acc: 0.3157\n",
      "Epoch: 15 loss: 1.8012,         acc: 0.3124\n",
      "Test_loss: 1.8148,                    Val_acc: 0.3134\n",
      "Epoch: 16 loss: 1.7672,         acc: 0.3204\n",
      "Test_loss: 1.7874,                    Val_acc: 0.3111\n",
      "Epoch: 17 loss: 1.7200,         acc: 0.3510\n",
      "Test_loss: 1.7473,                    Val_acc: 0.3272\n",
      "Epoch: 18 loss: 1.6794,         acc: 0.3695\n",
      "Test_loss: 1.7705,                    Val_acc: 0.3295\n",
      "Epoch: 19 loss: 1.7157,         acc: 0.3574\n",
      "Test_loss: 1.8711,                    Val_acc: 0.3226\n",
      "Epoch: 20 loss: 1.7287,         acc: 0.3528\n",
      "Test_loss: 1.9222,                    Val_acc: 0.2765\n",
      "Epoch: 21 loss: 1.5949,         acc: 0.4082\n",
      "Test_loss: 1.8125,                    Val_acc: 0.2811\n",
      "Epoch: 22 loss: 1.6367,         acc: 0.3915\n",
      "Test_loss: 1.8391,                    Val_acc: 0.3203\n",
      "Epoch: 23 loss: 1.5653,         acc: 0.4244\n",
      "Test_loss: 1.8206,                    Val_acc: 0.3226\n",
      "Epoch: 24 loss: 1.5246,         acc: 0.4607\n",
      "Test_loss: 1.8017,                    Val_acc: 0.3111\n",
      "Epoch: 25 loss: 1.5006,         acc: 0.4590\n",
      "Test_loss: 1.8211,                    Val_acc: 0.3041\n",
      "Epoch: 26 loss: 1.4013,         acc: 0.5069\n",
      "Test_loss: 1.7563,                    Val_acc: 0.3571\n",
      "Epoch: 27 loss: 1.3916,         acc: 0.4861\n",
      "Test_loss: 1.8764,                    Val_acc: 0.3318\n",
      "Epoch: 28 loss: 1.2956,         acc: 0.5352\n",
      "Test_loss: 1.8207,                    Val_acc: 0.3364\n",
      "Epoch: 29 loss: 1.2616,         acc: 0.5606\n",
      "Test_loss: 1.8533,                    Val_acc: 0.3433\n",
      "Epoch: 30 loss: 1.1447,         acc: 0.5889\n",
      "Test_loss: 1.8588,                    Val_acc: 0.3203\n",
      "Epoch: 31 loss: 1.0883,         acc: 0.6212\n",
      "Test_loss: 1.9187,                    Val_acc: 0.3687\n",
      "Epoch: 32 loss: 0.9970,         acc: 0.6663\n",
      "Test_loss: 1.9628,                    Val_acc: 0.3479\n",
      "Epoch: 33 loss: 0.9521,         acc: 0.6651\n",
      "Test_loss: 2.0251,                    Val_acc: 0.3502\n",
      "Epoch: 34 loss: 0.8819,         acc: 0.6963\n",
      "Test_loss: 2.0369,                    Val_acc: 0.3410\n",
      "Epoch: 35 loss: 0.8275,         acc: 0.7079\n",
      "Test_loss: 2.1664,                    Val_acc: 0.3203\n",
      "Epoch: 36 loss: 0.7834,         acc: 0.7182\n",
      "Test_loss: 2.1973,                    Val_acc: 0.3410\n",
      "Epoch: 37 loss: 0.7116,         acc: 0.7610\n",
      "Test_loss: 2.2262,                    Val_acc: 0.3134\n",
      "Epoch: 38 loss: 0.6861,         acc: 0.7615\n",
      "Test_loss: 2.2976,                    Val_acc: 0.3134\n",
      "Epoch: 39 loss: 0.6451,         acc: 0.7714\n",
      "Test_loss: 2.3504,                    Val_acc: 0.3548\n",
      "Epoch: 40 loss: 0.5821,         acc: 0.7904\n",
      "Test_loss: 2.4821,                    Val_acc: 0.3203\n",
      "Epoch: 41 loss: 0.5493,         acc: 0.8048\n",
      "Test_loss: 2.6407,                    Val_acc: 0.3111\n",
      "Epoch: 42 loss: 0.5000,         acc: 0.8245\n",
      "Test_loss: 2.6682,                    Val_acc: 0.3203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/TopoModelX/tutorials/hypergraph/allset_transformer_train.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f717569726b795f62616262616765227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/allset_transformer_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model(x_0s, incidence_1)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f717569726b795f62616262616765227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/allset_transformer_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_hat[train_mask], y[train_mask])\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f717569726b795f62616262616765227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/allset_transformer_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f717569726b795f62616262616765227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/allset_transformer_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f717569726b795f62616262616765227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/allset_transformer_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m epoch_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_0s = torch.tensor(x_0s)\n",
    "x_0s, incidence_1, y = (\n",
    "            x_0s.float().to(device),\n",
    "            incidence_1.float().to(device),\n",
    "            torch.tensor(y, dtype=torch.long).to(device),\n",
    "        )\n",
    "\n",
    "test_interval = 1\n",
    "num_epochs = 100\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    # Extract edge_index from sparse incidence matrix\n",
    "    # edge_index, _ = to_edge_index(incidence_1)\n",
    "    y_hat = model(x_0s, incidence_1)\n",
    "    loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    epoch_loss.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}, \\\n",
    "        acc: {acc_fn(y_hat[train_mask].argmax(1), y[train_mask]):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad(): \n",
    "           # y_hat = model(x_0s, incidence_1)\n",
    "            loss = loss_fn(y_hat[val_mask], y[val_mask])\n",
    "\n",
    "            print(f\"Test_loss: {loss:.4f}, \\\n",
    "                   Val_acc: {acc_fn(y_hat[val_mask].argmax(1), y[val_mask]):.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topomodelx.nn.hypergraph.allset import AllSet\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topomodelx.nn.hypergraph.allset import AllSet\n",
    "\n",
    "\n",
    "in_channels = x_0s.shape[1]\n",
    "hidden_channels = 64\n",
    "out_channels = len(torch.unique(y))\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = AllSet(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_layers=1,\n",
    "    mlp_num_layers=1,\n",
    ")\n",
    "model = model.to(device)\n",
    "# Optimizer and loss\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 1.9529,         acc: 0.1236\n",
      "Epoch: 2 loss: 1.9349,         acc: 0.2667\n",
      "Epoch: 3 loss: 1.8677,         acc: 0.2806\n",
      "Epoch: 4 loss: 2.0707,         acc: 0.2858\n",
      "Epoch: 5 loss: 1.8432,         acc: 0.2858\n",
      "Epoch: 6 loss: 1.8947,         acc: 0.2846\n",
      "Epoch: 7 loss: 1.9171,         acc: 0.2973\n",
      "Epoch: 8 loss: 1.9251,         acc: 0.2846\n",
      "Epoch: 9 loss: 1.9260,         acc: 0.2321\n",
      "Epoch: 10 loss: 1.9237,         acc: 0.2540\n",
      "Test_loss: 1.9265,                    Val_acc: 0.2350\n",
      "Epoch: 11 loss: 1.9183,         acc: 0.2494\n",
      "Epoch: 12 loss: 1.9095,         acc: 0.2286\n",
      "Epoch: 13 loss: 1.8956,         acc: 0.2350\n",
      "Epoch: 14 loss: 1.8771,         acc: 0.2061\n",
      "Epoch: 15 loss: 1.8649,         acc: 0.1940\n",
      "Epoch: 16 loss: 1.8560,         acc: 0.2246\n",
      "Epoch: 17 loss: 1.8444,         acc: 0.2598\n",
      "Epoch: 18 loss: 1.8181,         acc: 0.2627\n",
      "Epoch: 19 loss: 1.8011,         acc: 0.2823\n",
      "Epoch: 20 loss: 1.7956,         acc: 0.2852\n",
      "Test_loss: 1.8108,                    Val_acc: 0.3041\n",
      "Epoch: 21 loss: 1.7834,         acc: 0.2864\n",
      "Epoch: 22 loss: 1.7577,         acc: 0.2858\n",
      "Epoch: 23 loss: 1.7415,         acc: 0.2858\n",
      "Epoch: 24 loss: 1.7264,         acc: 0.2858\n",
      "Epoch: 25 loss: 1.7123,         acc: 0.2835\n",
      "Epoch: 26 loss: 1.7034,         acc: 0.2766\n",
      "Epoch: 27 loss: 1.6933,         acc: 0.2679\n",
      "Epoch: 28 loss: 1.6794,         acc: 0.2783\n",
      "Epoch: 29 loss: 1.6829,         acc: 0.2841\n",
      "Epoch: 30 loss: 1.6637,         acc: 0.2852\n",
      "Test_loss: 1.8413,                    Val_acc: 0.3018\n",
      "Epoch: 31 loss: 1.6473,         acc: 0.2858\n",
      "Epoch: 32 loss: 1.6462,         acc: 0.2858\n",
      "Epoch: 33 loss: 1.6268,         acc: 0.2858\n",
      "Epoch: 34 loss: 1.6182,         acc: 0.2858\n",
      "Epoch: 35 loss: 1.5960,         acc: 0.2870\n",
      "Epoch: 36 loss: 1.5999,         acc: 0.2875\n",
      "Epoch: 37 loss: 1.5749,         acc: 0.2973\n",
      "Epoch: 38 loss: 1.5567,         acc: 0.2973\n",
      "Epoch: 39 loss: 1.5320,         acc: 0.3066\n",
      "Epoch: 40 loss: 1.5222,         acc: 0.3447\n",
      "Test_loss: 2.2088,                    Val_acc: 0.3180\n",
      "Epoch: 41 loss: 1.5000,         acc: 0.4099\n",
      "Epoch: 42 loss: 1.4902,         acc: 0.4140\n",
      "Epoch: 43 loss: 1.4689,         acc: 0.4088\n",
      "Epoch: 44 loss: 1.4653,         acc: 0.4249\n",
      "Epoch: 45 loss: 1.4276,         acc: 0.4261\n",
      "Epoch: 46 loss: 1.4370,         acc: 0.4267\n",
      "Epoch: 47 loss: 1.4056,         acc: 0.4515\n",
      "Epoch: 48 loss: 1.4119,         acc: 0.4463\n",
      "Epoch: 49 loss: 1.3677,         acc: 0.4544\n",
      "Epoch: 50 loss: 1.3669,         acc: 0.4480\n",
      "Test_loss: 3.1498,                    Val_acc: 0.3018\n",
      "Epoch: 51 loss: 1.3442,         acc: 0.4654\n",
      "Epoch: 52 loss: 1.3433,         acc: 0.4630\n",
      "Epoch: 53 loss: 1.3118,         acc: 0.4769\n",
      "Epoch: 54 loss: 1.3083,         acc: 0.4746\n",
      "Epoch: 55 loss: 1.2765,         acc: 0.4838\n",
      "Epoch: 56 loss: 1.2741,         acc: 0.4942\n",
      "Epoch: 57 loss: 1.2464,         acc: 0.4931\n",
      "Epoch: 58 loss: 1.2437,         acc: 0.5040\n",
      "Epoch: 59 loss: 1.2314,         acc: 0.5069\n",
      "Epoch: 60 loss: 1.2433,         acc: 0.5266\n",
      "Test_loss: 3.3781,                    Val_acc: 0.2949\n",
      "Epoch: 61 loss: 1.2146,         acc: 0.5225\n",
      "Epoch: 62 loss: 1.1880,         acc: 0.5479\n",
      "Epoch: 63 loss: 1.1797,         acc: 0.5381\n",
      "Epoch: 64 loss: 1.1512,         acc: 0.5577\n",
      "Epoch: 65 loss: 1.1596,         acc: 0.5577\n",
      "Epoch: 66 loss: 1.1456,         acc: 0.5497\n",
      "Epoch: 67 loss: 1.1509,         acc: 0.5572\n",
      "Epoch: 68 loss: 1.1187,         acc: 0.5699\n",
      "Epoch: 69 loss: 1.1128,         acc: 0.5837\n",
      "Epoch: 70 loss: 1.1218,         acc: 0.5745\n",
      "Test_loss: 3.7407,                    Val_acc: 0.2949\n",
      "Epoch: 71 loss: 1.1221,         acc: 0.5826\n",
      "Epoch: 72 loss: 1.1162,         acc: 0.5739\n",
      "Epoch: 73 loss: 1.1068,         acc: 0.5820\n",
      "Epoch: 74 loss: 1.0719,         acc: 0.5958\n",
      "Epoch: 75 loss: 1.0542,         acc: 0.5976\n",
      "Epoch: 76 loss: 1.0443,         acc: 0.5947\n",
      "Epoch: 77 loss: 1.0627,         acc: 0.5976\n",
      "Epoch: 78 loss: 1.0244,         acc: 0.6022\n",
      "Epoch: 79 loss: 1.0041,         acc: 0.6132\n",
      "Epoch: 80 loss: 0.9969,         acc: 0.6236\n",
      "Test_loss: 4.3202,                    Val_acc: 0.2903\n",
      "Epoch: 81 loss: 1.0078,         acc: 0.6288\n",
      "Epoch: 82 loss: 0.9786,         acc: 0.6264\n",
      "Epoch: 83 loss: 0.9896,         acc: 0.6241\n",
      "Epoch: 84 loss: 0.9532,         acc: 0.6380\n",
      "Epoch: 85 loss: 0.9531,         acc: 0.6282\n",
      "Epoch: 86 loss: 0.9312,         acc: 0.6380\n",
      "Epoch: 87 loss: 0.9289,         acc: 0.6501\n",
      "Epoch: 88 loss: 0.9323,         acc: 0.6467\n",
      "Epoch: 89 loss: 0.9228,         acc: 0.6443\n",
      "Epoch: 90 loss: 0.9243,         acc: 0.6507\n",
      "Test_loss: 4.6642,                    Val_acc: 0.2650\n",
      "Epoch: 91 loss: 0.9103,         acc: 0.6513\n",
      "Epoch: 92 loss: 0.9070,         acc: 0.6565\n",
      "Epoch: 93 loss: 0.8845,         acc: 0.6542\n",
      "Epoch: 94 loss: 0.8737,         acc: 0.6536\n",
      "Epoch: 95 loss: 0.8587,         acc: 0.6767\n",
      "Epoch: 96 loss: 0.8309,         acc: 0.6842\n",
      "Epoch: 97 loss: 0.8485,         acc: 0.6801\n",
      "Epoch: 98 loss: 0.8357,         acc: 0.6865\n",
      "Epoch: 99 loss: 0.8122,         acc: 0.7027\n",
      "Epoch: 100 loss: 0.8095,         acc: 0.6992\n",
      "Test_loss: 6.1745,                    Val_acc: 0.2903\n"
     ]
    }
   ],
   "source": [
    "test_interval = 10\n",
    "num_epochs = 100\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    # Extract edge_index from sparse incidence matrix\n",
    "    # edge_index, _ = to_edge_index(incidence_1)\n",
    "    y_hat = model(x_0s, incidence_1)\n",
    "    loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    epoch_loss.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}, \\\n",
    "        acc: {acc_fn(y_hat[train_mask].argmax(1), y[train_mask]):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad(): \n",
    "           # y_hat = model(x_0s, incidence_1)\n",
    "            loss = loss_fn(y_hat[val_mask], y[val_mask])\n",
    "\n",
    "            print(f\"Test_loss: {loss:.4f}, \\\n",
    "                   Val_acc: {acc_fn(y_hat[val_mask].argmax(1), y[val_mask]):.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9480, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
